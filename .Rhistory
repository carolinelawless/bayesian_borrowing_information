epsilons_H0_1
epsilons_H0_2
threshold <- (threshold1 + threshold2)/2
threshold <- threshold2 #the most conservative threshold
# Simulate under H1 (all params different)
params_H1_1 <- c(0.5, 0.55, 0.6, 0.65, 0.7)  # all params different
S_H1_1_all <- simulate_S(params_H1_1, n)
S_H1_1 <- S_H1_1_all[[1]]
epsilons_H1_1 <- S_H1_1_all[[2]]
epsilons_H1_1 #0.75, 0.68, 0.65, 0.64
# Check how often S_H1 < threshold to estimate power
length(which(S_H1_1 < threshold))/length(S_H1_1)
# Simulate under H1 (last param different)
params_H1_2 <- c(0.5, 0.5, 0.5, 0.5, 0.7)  # last param different
S_H1_2_all <- simulate_S(params_H1_2, n)
S_H1_2 <- S_H1_2_all[[1]]
epsilons_H1_2 <- S_H1_2_all[[2]]
epsilons_H1_2 #0.77, 0.78, 0.79, 0.23
# Check how often S_H1 < threshold to estimate power
length(which(S_H1_1 < threshold))/length(S_H1_2)
length(which(S_H1_2 < threshold))/length(S_H0_2)
mean(S_H1_1)
mean(S_H1_2)
remove(list = ls())
library(rjags)
library(coda)
library(tictoc)
params <- rep(0.5, 5)
n <- 1e2
model_string <- "
model {
for (j in 1:J) {
for (i in 1:n) {
y[j,i] ~ dbern(params[j])
}
}
for (j in 1:J) {
logit(params[j]) <- theta[j]
}
theta[1] ~ dnorm(0, 0.01)
for (j in 2:J) {
theta[j] ~ dnorm(theta[j-1], tau[j])
}
for (j in 2:J) {
sigma[j] ~ dunif(0, 5)
tau[j] <- pow(sigma[j], -2)
}
}
"
jags_sampler <- function(params, n, model_string){
J <- length(params)
# --- Simulate data ---
y <- matrix(NA, nrow = J, ncol = n)
for(j in 1:J){
y[j, ] <- rbinom(n, 1, params[j])
}
# --- JAGS data list ---
data_jags <- list(
y = y,
n = n,
J = J
)
# --- Build model ---
model <- jags.model(
textConnection(model_string),
data = data_jags,
n.chains = 3,
n.adapt = 1000
)
# Burn-in
update(model, 1000)
# Sample
samples <- coda.samples(
model,
variable.names = c(
"params",
"theta",
"tau"
),
n.iter = 10000
)
samples_mat <- as.matrix(samples)
return(samples_mat)
}
simulate_S <- function(params, n, n_sim = 10){
S_vals <- numeric(n_sim)
J <- length(params)
for(i in 1:n_sim){
print(i)
samples_mat <- jags_sampler(params, n, model_string)
S <- 0
for(j in 2:J){
S <- S + mean(samples_mat[, paste0("tau[",j,"]")])
}
S_vals[i] <- S
}
return(S_vals)
}
remove(list = ls())
library(rjags)
library(coda)
library(tictoc)
params <- rep(0.5, 5)
n <- 1e2
model_string <- "
model {
for (j in 1:J) {
for (i in 1:n) {
y[j,i] ~ dbern(params[j])
}
}
for (j in 1:J) {
logit(params[j]) <- theta[j]
}
theta[1] ~ dnorm(0, 0.01)
for (j in 2:J) {
theta[j] ~ dnorm(theta[j-1], tau[j])
}
for (j in 2:J) {
sigma[j] ~ dunif(0, 5)
tau[j] <- pow(sigma[j], -2)
}
}
"
jags_sampler <- function(params, n, model_string){
J <- length(params)
# --- Simulate data ---
y <- matrix(NA, nrow = J, ncol = n)
for(j in 1:J){
y[j, ] <- rbinom(n, 1, params[j])
}
# --- JAGS data list ---
data_jags <- list(
y = y,
n = n,
J = J
)
# --- Build model ---
model <- jags.model(
textConnection(model_string),
data = data_jags,
n.chains = 3,
n.adapt = 1000
)
# Burn-in
update(model, 1000)
# Sample
samples <- coda.samples(
model,
variable.names = c(
"params",
"theta",
"tau"
),
n.iter = 10000
)
samples_mat <- as.matrix(samples)
return(samples_mat)
}
simulate_S <- function(params, n, n_sim = 1e2){
S_vals <- numeric(n_sim)
J <- length(params)
for(i in 1:n_sim){
print(i)
samples_mat <- jags_sampler(params, n, model_string)
S <- 0
for(j in 2:J){
S <- S + mean(samples_mat[, paste0("tau[",j,"]")])
}
S_vals[i] <- S
}
return(S_vals)
}
t1 <- proc.time()
# Simulate under H0 (same params for all datasets)
S_H0 <- simulate_S(rep(0.5, 5), n)
threshold <- quantile(S_H0, 0.05)  # 5% quantile for 5% false positive rate
# Simulate under H1 (last param different)
params_H1 <- c(0.5, 0.5, 0.5, 0.5, 0.7)  # last dataset different
S_H1 <- simulate_S(params_H1, n)
# You can check how often S_H1 < threshold to estimate power
length(which(S_H1 < threshold))/length(S_H1)
t2 <- proc.time()
t2 - t1
length(which(S_H0 < threshold)/length(S_H0))
length(which(S_H0 < threshold))/length(S_H0)
length(which(S_H1 < threshold))/length(S_H0)
n
470/60
remove(list = ls())
library(rjags)
library(coda)
library(tictoc)
model_string <- "
model {
for (j in 1:J) {
for (i in 1:n) {
y[j,i] ~ dbern(params[j])
}
}
for (j in 1:J) {
logit(params[j]) <- theta[j]
}
theta[1] ~ dnorm(0, 0.01)
for (j in 2:J) {
theta[j] ~ dnorm(theta[j-1], tau[j])
}
for (j in 2:J) {
sigma[j] ~ dunif(0, 5)
tau[j] <- pow(sigma[j], -2)
}
}
"
jags_sampler <- function(params, n, model_string){
J <- length(params)
# --- Simulate data ---
y <- matrix(NA, nrow = J, ncol = n)
for(j in 1:J){
y[j, ] <- rbinom(n, 1, params[j])
}
# --- JAGS data list ---
data_jags <- list(
y = y,
n = n,
J = J
)
# --- Build model ---
model <- jags.model(
textConnection(model_string),
data = data_jags,
n.chains = 3,
n.adapt = 1000
)
# Burn-in
update(model, 1000)
# Sample
samples <- coda.samples(
model,
variable.names = c(
"params",
"theta",
"tau"
),
n.iter = 10000
)
samples_mat <- as.matrix(samples)
return(samples_mat)
}
params
samples <- jags_sampler(rep(0.5, 5), 1e2, model_string)
samples_mat <- as.matrix(samples)
colnames(samples_mat)
# Extract posterior samples for theta[J]
theta_cols <- grep("^theta\\[", colnames(samples_mat))
theta_cols
theta_samples <- samples_mat[, theta_cols, drop = FALSE]
head(theta_samples)
J <- ncol(theta_samples)  # number of historical datasets
theta_1 <- theta_samples[, 1]   # latent parameter for first historical version
# Extract tau[J+1] if present, else create a prior for predictive variance
tau_cols <- grep("^tau\\[", colnames(samples_mat))
tau_samples <- samples_mat[, tau_cols, drop = FALSE]
head(tau_samples)
head(samples_mat)
# If your model does NOT include tau[J+1] (e.g., only tau[2:J]),
# then use the last tau as commensurate precision for predictive step.
tau_for_pred <- tau_samples[, ncol(tau_samples)]
# Number of posterior draws
S <- length(theta_J)
plogis(0.5)
plogis(0)
plogis(100)
plogis(1000)
plogis(10)
theta_samples <- samples_mat[, theta_cols, drop = FALSE]
head(theta_samples)
plogis(theta_samples)
head(plogis(theta_samples))
theta_samples
head(theta_samples)
head(plogis(theta_samples))
max(theta_samples)
min(theta_samples)
colnames(samples_mat)
head(samples_mat)
p_cols <- grep("^params\\[", colnames(samples_mat))
p_samples <- samples_mat[, p_cols, drop = FALSE]
p_samples
head(p_samples)
p_cols[,1]
p_cols <- grep("^params\\[", colnames(samples_mat))
p_samples <- samples_mat[, p_cols, drop = FALSE]
p1 <- p_samples[,1]
p1
is.vector(p1)
pJ <- p_samples[,ncol(p_samples)]
p_cols <- grep("^params\\[", colnames(samples_mat))
p_samples <- samples_mat[, p_cols, drop = FALSE]
p1 <- p_samples[,1]
pK <- p_samples[,ncol(p_samples)]
diffs <- numeric(n_pred)
for(b in 1:npred){
y1_rep <- rbinom(n, 1, p1)
yK_rep <- rbinom(n, 1, pK)
diffs[b] <- mean(yK_rep) - mean(y1_rep)
}
n_pred = 2e3
p_cols <- grep("^params\\[", colnames(samples_mat))
p_samples <- samples_mat[, p_cols, drop = FALSE]
p1 <- p_samples[,1]
pK <- p_samples[,ncol(p_samples)]
diffs <- numeric(n_pred)
for(b in 1:npred){
y1_rep <- rbinom(n, 1, p1)
yK_rep <- rbinom(n, 1, pK)
diffs[b] <- mean(yK_rep) - mean(y1_rep)
}
for(b in 1:n_pred){
y1_rep <- rbinom(n, 1, p1)
yK_rep <- rbinom(n, 1, pK)
diffs[b] <- mean(yK_rep) - mean(y1_rep)
}
n <- 1e2
for(b in 1:n_pred){
y1_rep <- rbinom(n, 1, p1)
yK_rep <- rbinom(n, 1, pK)
diffs[b] <- mean(yK_rep) - mean(y1_rep)
}
diffs
mean(diffs)
p1
sample(p1, 1)
sample(p1, 1)
sample(p1, 1)
sample(p1, 1)
samples_mat
posterior_predictive_diff <- function(samples_mat, n, n_pred = 2e3) {
p_cols <- grep("^params\\[", colnames(samples_mat))
p_samples <- samples_mat[, p_cols, drop = FALSE]
p1 <- p_samples[,1]
pK <- p_samples[,ncol(p_samples)]
diffs <- numeric(n_pred)
for(b in 1:n_pred){
p1_samp <- sample(p1, 1)
pK_samp <- sample(pK, 1)
y1_rep <- rbinom(n, 1, p1_samp)
yK_rep <- rbinom(n, 1, pK_samp)
diffs[b] <- mean(yK_rep) - mean(y1_rep)
}
return(diffs)
}
posterior_predictive_diff(samples_mat, n)
res1 <- posterior_predictive_diff(samples_mat, n)
mean(res1)
simulate_S_parallel <- function(params, n, n_sim = 100) {
cl <- makeCluster(detectCores() - 1)  # leave 1 core free
clusterExport(cl, c("params", "n", "model_string", "jags_sampler"))
S_vals <- parSapply(cl, 1:n_sim, function(i) {
samples_mat <- jags_sampler(params, n, model_string)
J <- length(params)
S <- 0
for(j in 2:J){
S <- S + mean(samples_mat[, paste0("tau[", j, "]")])
}
return(S)
})
stopCluster(cl)
return(S_vals)
}
simulate_S_parallel(rep(0.5,5), 1e2)
library(parallel)
simulate_S_parallel <- function(params, n, n_sim = 100) {
cl <- makeCluster(detectCores() - 1)  # leave 1 core free
clusterExport(cl, c("params", "n", "model_string", "jags_sampler"))
S_vals <- parSapply(cl, 1:n_sim, function(i) {
samples_mat <- jags_sampler(params, n, model_string)
J <- length(params)
S <- 0
for(j in 2:J){
S <- S + mean(samples_mat[, paste0("tau[", j, "]")])
}
return(S)
})
stopCluster(cl)
return(S_vals)
}
simulate_S_parallel(rep(0.5,5), 1e2)
simulate_S_parallel(rep(0.5,5), 1e2)
params <- rep(0.5, 5)
simulate_S_parallel <- function(params, n, n_sim = 100) {
cl <- makeCluster(detectCores() - 1)  # leave 1 core free
clusterExport(cl, c("params", "n", "model_string", "jags_sampler"))
S_vals <- parSapply(cl, 1:n_sim, function(i) {
samples_mat <- jags_sampler(params, n, model_string)
J <- length(params)
S <- 0
for(j in 2:J){
S <- S + mean(samples_mat[, paste0("tau[", j, "]")])
}
return(S)
})
stopCluster(cl)
return(S_vals)
}
simulate_S_parallel(params, 1e2)
cl <- makeCluster(detectCores() - 1)  # leave 1 core free
clusterExport(cl, c("params", "n", "model_string", "jags_sampler"))
S_vals <- parSapply(cl, 1:n_sim, function(i) {
samples_mat <- jags_sampler(params, n, model_string)
J <- length(params)
S <- 0
for(j in 2:J){
S <- S + mean(samples_mat[, paste0("tau[", j, "]")])
}
return(S)
})
n_sim <- 1e2
S_vals <- parSapply(cl, 1:n_sim, function(i) {
samples_mat <- jags_sampler(params, n, model_string)
J <- length(params)
S <- 0
for(j in 2:J){
S <- S + mean(samples_mat[, paste0("tau[", j, "]")])
}
return(S)
})
remove(list = ls())
library(rjags)
library(coda)
library(parallel)
model_string <- "
model {
for (j in 1:J) {
for (i in 1:n) {
y[j,i] ~ dbern(params[j])
}
}
for (j in 1:J) {
logit(params[j]) <- theta[j]
}
theta[1] ~ dnorm(0, 0.01)
for (j in 2:J) {
theta[j] ~ dnorm(theta[j-1], tau[j])
}
for (j in 2:J) {
sigma[j] ~ dunif(0, 5)
tau[j] <- pow(sigma[j], -2)
}
}
"
#jags_sampler <- function(params, n, model_string){
jags_sampler <- function(y, n, model_string){
J <- length(params)
# --- Simulate data ---
y <- matrix(NA, nrow = J, ncol = n)
for(j in 1:J){
y[j, ] <- rbinom(n, 1, params[j])
}
# --- JAGS data list ---
data_jags <- list(
y = y,
n = n,
J = J
)
# --- Build model ---
model <- jags.model(
textConnection(model_string),
data = data_jags,
n.chains = 3,
n.adapt = 1000
)
# Burn-in
update(model, 1000)
# Sample
samples <- coda.samples(
model,
variable.names = c(
"params",
"theta",
"tau"
),
n.iter = 10000
)
samples_mat <- as.matrix(samples)
return(samples_mat)
}
# --- Simulate data ---
params <- rep(0.5, 5)
y <- matrix(NA, nrow = J, ncol = n)
for(j in 1:J){
y[j, ] <- rbinom(n, 1, params[j])
}
# --- Simulate data ---
params <- rep(0.5, 5)
y <- matrix(NA, nrow = J, ncol = n)
# --- Simulate data ---
params <- rep(0.5, 5)
y <- matrix(NA, nrow = J, ncol = n)
# --- Simulate data ---
params <- rep(0.5, 5)
J <- length(params)
y <- matrix(NA, nrow = J, ncol = n)
# --- Simulate data ---
params <- rep(0.5, 5)
J <- length(params)
n <- 1e2
y <- matrix(NA, nrow = J, ncol = n)
for(j in 1:J){
y[j, ] <- rbinom(n, 1, params[j])
}
y
jags_sampler(y, n, model_string)
